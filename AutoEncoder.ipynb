{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder (VAE) PyTorch implementation\n",
    "Based on the paper by Larsen et al.: \"Autoencoding beyond pixels using a learned similarity metric\" - https://arxiv.org/abs/1512.09300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library options\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# Get CPU or GPU device for NN\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def airfoil_info(airfoil_df, plot_airfoil, mach=0.1, reynolds=1e5):\n",
    "    \"\"\"\n",
    "    Returns a plot of a NACA airfoil's shape, Cl and Cd between alpha = -10 and alpha = 10 degrees. \n",
    "    Assumes 15 points for upper surface and 15 points for lower surface.\n",
    "    Inputs:\n",
    "        - airfoil_df: pandas dataframe with airfoil data\n",
    "        - plot_airfoil: index that specifies which airfoil to plot\n",
    "    Outputs:\n",
    "        - Plot of airfoil shape, Cl and Cd\n",
    "    \"\"\"\n",
    "    # X coordinates\n",
    "    x = [0.5*(1-np.cos(ang)) for ang in np.linspace(0,np.pi,17)]\n",
    "    aux_x = list(reversed([0.5*(1-np.cos(ang)) for ang in np.linspace(0,np.pi,17)[1:16]]))\n",
    "    [x.append(i) for i in aux_x]\n",
    "    x.append(0)\n",
    "    \n",
    "    first_coord_list = airfoil_df.loc[(airfoil_df['MachNumber'] == mach) & (airfoil_df['ReynoldsNumber'] == reynolds)]['yU_1'].unique()\n",
    "    plot_airfoil_df = airfoil_df.loc[(airfoil_df['MachNumber'] == mach) & (airfoil_df['ReynoldsNumber'] == reynolds) & (airfoil_df['yU_1']==first_coord_list[plot_airfoil])]\n",
    "    \n",
    "    # Y coordinates\n",
    "    y = []\n",
    "    origin = (plot_airfoil_df.iloc[1][0]+plot_airfoil_df.iloc[1][15])/2\n",
    "    y.append(origin)\n",
    "    [y.append(j) for j in plot_airfoil_df.iloc[1][0:15].values.tolist()]\n",
    "    y.append(0)\n",
    "    aux_y = list(reversed(plot_airfoil_df.iloc[1][15:30].values.tolist()))\n",
    "    [y.append(k) for k in aux_y]\n",
    "    y.append(origin)\n",
    "    \n",
    "    # Cl, Cd, alphas\n",
    "    Cl = plot_airfoil_df['Cl'].values.tolist()[0:21]\n",
    "    Cd = plot_airfoil_df['Cd'].values.tolist()[0:21]\n",
    "    alphas = np.linspace(-10,10,len(Cl))\n",
    "    \n",
    "    # Airfoil plot\n",
    "    plot1 = plt.subplot2grid((2,2), (0,0), colspan = 2)\n",
    "    plot2 = plt.subplot2grid((2,2), (1,0))\n",
    "    plot3 = plt.subplot2grid((2,2), (1,1))\n",
    "    \n",
    "    plot1.plot(x, y)\n",
    "    plot1.set_xlim([-0.1,1.1])\n",
    "    plot1.set_ylim([np.min(y)-0.2*np.abs(np.min(y)),np.max(y)+0.2*np.abs(np.max(y))])\n",
    "    plot1.set_ylabel('$y/c$')\n",
    "    plot1.set_xlabel('$x/c$') \n",
    "    plot1.set_title('Airfoil plot', fontsize=16)\n",
    "    \n",
    "    plot2.plot(alphas, Cl)\n",
    "    plot2.set_xlim([-10,10])\n",
    "    plot2.set_ylim([np.min(Cl)-0.1*np.abs(np.min(Cl)),np.max(Cl)+0.1*np.abs(np.max(Cl))])\n",
    "    plot2.set_ylabel('$C_L$')\n",
    "    plot2.set_xlabel('$\\\\alpha$ [$^\\\\circ$]') \n",
    "    \n",
    "    plot3.plot(alphas, Cd)\n",
    "    plot3.set_xlim([-10,10])\n",
    "    plot3.set_ylim([np.min(Cd)-0.1*np.abs(np.min(Cd)),np.max(Cd)+0.1*np.abs(np.max(Cd))])\n",
    "    plot3.set_ylabel('$C_D$')\n",
    "    plot3.set_xlabel('$\\\\alpha$ [$^\\\\circ$]') \n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def airfoil_plot(airfoil_coords, fig=None, label=None):\n",
    "    \"\"\"\n",
    "    Returns a plot of an airfoil. Used to visualize output of the optimizer. \n",
    "    Assumes 15 points for upper surface and 15 points for lower surface, with cosine spacing.\n",
    "    Inputs:\n",
    "        - airfoil_coords: pandas DataFrame with airfoil coordinates and other parameters\n",
    "    Outputs:\n",
    "        - Plot of airfoil shape\n",
    "    \"\"\"\n",
    "    if fig==None:\n",
    "        fig = plt.subplot2grid((1,3), (0,0), colspan = 3)\n",
    "    # X coordinates\n",
    "    x = [0.5*(1-np.cos(ang)) for ang in np.linspace(0,np.pi,17)]\n",
    "    aux_x = list(reversed([0.5*(1-np.cos(ang)) for ang in np.linspace(0,np.pi,17)[1:16]]))\n",
    "    [x.append(i) for i in aux_x]\n",
    "    x.append(0)\n",
    "    \n",
    "    # Y coordinates\n",
    "    y = []\n",
    "    origin = (airfoil_coords.iloc[0][0]+airfoil_coords.iloc[0][15])/2\n",
    "    y.append(origin)\n",
    "    [y.append(j) for j in airfoil_coords.iloc[0][0:15].values.tolist()]\n",
    "    y.append(0)\n",
    "    aux_y = list(reversed(airfoil_coords.iloc[0][15:30].values.tolist()))\n",
    "    [y.append(k) for k in aux_y]\n",
    "    y.append(origin)\n",
    "    \n",
    "    # Airfoil plot     \n",
    "    fig.plot(x, y, label = label)\n",
    "    fig.set_xlim([-0.1,1.1])\n",
    "    fig.set_ylim([-0.2,0.3])\n",
    "    fig.set_ylabel('$y/c$')\n",
    "    fig.set_xlabel('$x/c$') \n",
    "    fig.set_title('Airfoil plot', fontsize=16)\n",
    "    fig.legend()\n",
    "    if fig==None:\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def torch_test_split(X, y, test_size=0.2, seed=1234):\n",
    "    \"\"\"\n",
    "    Returns a train and test set in PyTorch tensor format from a numpy array dataset.\n",
    "    Inputs:\n",
    "        - X: numpy array with input data. Each row is a training/testing sample and each column is a feature.\n",
    "        - y: numpy array with output data. Each row is a training/testing sample and each column is an output.\n",
    "        - test_size: proportion of the dataset to be used as test set.\n",
    "        - seed: random seed for reproducibility.\n",
    "    Outputs:\n",
    "        - training_data: PyTorch tensor with training data.\n",
    "        - test_data: PyTorch tensor with test data.\n",
    "    \"\"\"\n",
    "    X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "    X_train = torch.from_numpy(X_train_0).float()\n",
    "    X_test = torch.from_numpy(X_test_0).float()\n",
    "    y_train = torch.from_numpy(y_train_0).float()\n",
    "    y_test = torch.from_numpy(y_test_0).float()\n",
    "    training_data = []\n",
    "    testing_data = []\n",
    "    for i in range(len(X_train)):\n",
    "        training_data.append((X_train[i], y_train[i]))\n",
    "    for i in range(len(X_test)):\n",
    "        testing_data.append((X_test[i], y_test[i]))\n",
    "    return training_data, testing_data\n",
    "\n",
    "def normalize_data (data, scaler):\n",
    "    \"\"\"\n",
    "    Normalizes neural network inputs and outputs.\n",
    "    Inputs:\n",
    "        - data: data to be normalized. [np.array / pd.DataFrame]\n",
    "        - scaler: pre-fitted scaler object.\n",
    "    Outputs:\n",
    "        - normalized data. [pd.DataFrame]\n",
    "    \"\"\"\n",
    "    if type(data) == pd.DataFrame:\n",
    "        data = data.to_numpy().reshape(-1,scaler.n_features_in_)\n",
    "    elif type(data) == np.ndarray:\n",
    "        data = data.reshape(-1,scaler.n_features_in_)\n",
    "    else:\n",
    "        raise(TypeError('Input data must be either a pd.DataFrame or a np.ndarray'))\n",
    "    norm_data = pd.DataFrame(data = scaler.transform(data), columns = scaler.feature_names_in_)\n",
    "    return norm_data\n",
    "\n",
    "def denormalize_data (data, scaler):\n",
    "    \"\"\"\n",
    "    Denormalizes neural network inputs and outputs.\n",
    "    Inputs:\n",
    "        - data: data to be denormalized. [np.array / pd.DataFrame]\n",
    "        - scaler: pre-fitted scaler object.\n",
    "    Outputs:\n",
    "        - denormalized data. [pd.DataFrame]\n",
    "    \"\"\"\n",
    "    if type(data) == pd.DataFrame:\n",
    "        data = data.to_numpy().reshape(-1,scaler.n_features_in_)\n",
    "    elif type(data) == np.ndarray:\n",
    "        data = data.reshape(-1,scaler.n_features_in_)\n",
    "    else:\n",
    "        raise(TypeError('Input data must be either a pd.DataFrame or a np.ndarray'))\n",
    "    denorm_data = pd.DataFrame(data = scaler.inverse_transform(data), columns = scaler.feature_names_in_)\n",
    "    return denorm_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input dataset (.csv) name and path\n",
    "data_folder = './data/'\n",
    "dataset_name = 'NACA4Digit_Dataset15Point.csv'\n",
    "\n",
    "# Import dataset\n",
    "airfoil_df = pd.read_csv(data_folder + dataset_name)\n",
    "airfoil_df = airfoil_df.drop('Unnamed: 0', axis=1)    # Remove first column, counter\n",
    "\n",
    "# Get rid of duplicates\n",
    "airfoil_df = airfoil_df.drop_duplicates(subset=['yU_1', 'yL_1', 'ReynoldsNumber', 'MachNumber', 'alpha', 'Cl','Cd','Cm'])\n",
    "\n",
    "airfoil_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder model\n",
    "In this section, an MLP based encoder-decoder network will be created and trained to process airfoil coordinates.\n",
    "\n",
    "**Inputs**\n",
    "- Upper surface coordinates (15)\n",
    "- Lower surface coordinates (15)\n",
    "\n",
    "**Outputs**\n",
    "- Approximately the same coordinates (network will be trained to do so)\n",
    "\n",
    "There will be a layer in the middle that will encode the _latent features_ of the set. This is akin to a parameterization method, with arbitrary parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scaler fitting\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(airfoil_df)\n",
    "\n",
    "# Assemble a DataFrame with all the minimum and maximum values of each column\n",
    "# For normalization and de-normalization. Gives an idea of the bounds.\n",
    "scaler_bounds = pd.DataFrame(data = np.stack([scaler.feature_names_in_, scaler.data_min_, scaler.data_max_], axis=1), columns=['property', 'min', 'max'])\n",
    "\n",
    "# Data normalization\n",
    "airfoil_df_norm = normalize_data(airfoil_df, scaler)\n",
    "\n",
    "# Input and \"output\" features\n",
    "# Input and output features are both the same for this dataset.\n",
    "X = airfoil_df_norm.drop(['Cl', 'Cd', 'Cm', 'ReynoldsNumber', 'MachNumber', 'alpha'], axis=1).values\n",
    "\n",
    "# Data tensors\n",
    "training_data, test_data = torch_test_split(X, X, test_size=0.2)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 1024\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape} {y.dtype}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "num_features = 3\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder-decoder neural network based on MLP layers. \n",
    "    Widths = [64, 32, num_features] for the encoder and [32, 64] for the decoder.\n",
    "    Activation : ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(30, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_features)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(num_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self,x):\n",
    "        decoded = self.decoder(x)\n",
    "        return decoded\n",
    "\n",
    "model = EncoderDecoder(num_features).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "# As per Moin et al. (2021), use MSE loss and Adam optimizer.\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test functions\n",
    "def train(dataloader, model, loss_fn, optimizer, loss_output = None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "    if loss_output is not None:\n",
    "        loss_output.append(loss.item())\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "epochs = 150\n",
    "loss_plot = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer, loss_output = loss_plot)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Loss plot\n",
    "plt.plot(range(1, epochs+1), loss_plot)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch') \n",
    "plt.title('Training loss', fontsize=16)\n",
    "plt.xlim([0,epochs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "model_name = \"AE_MLP128_30_\"+str(num_features)+\".pth\"\n",
    "if save_model:\n",
    "    trained_root = \"./trained_models/\"\n",
    "    \n",
    "    model_path = trained_root + model_name\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved PyTorch Model State to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For these sections, the CPU will be used\n",
    "device = 'cpu'\n",
    "model = EncoderDecoder(num_features).to(device)\n",
    "model_root = \"./trained_models/\"\n",
    "model_name = \"AE_MLP128_30_\"+str(num_features)+\".pth\"\n",
    "\n",
    "model_path = model_root + model_name\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Making predictions on random airfoils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxpred = np.random.randint(0, len(test_data))\n",
    "x, y = test_data[idxpred][0].to(device), test_data[idxpred][1].to(device)\n",
    "dummy = np.array([0, 0, 0, 0, 0, 0]) # Dummy variable to complete the dataset rows to 36 for plotting\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    reconstructed = denormalize_data(np.concatenate((pred.cpu().numpy(), dummy), axis=0),scaler)\n",
    "    original = denormalize_data(np.concatenate((x.cpu().numpy(), dummy), axis=0),scaler)\n",
    "    plot1 = plt.subplot2grid((1,3), (0,0), colspan = 3)\n",
    "    print(\"------ Comparison ------\")\n",
    "    airfoil_plot(original, fig=plot1, label=\"Original\")\n",
    "    airfoil_plot(reconstructed, fig=plot1, label=\"Reconstructed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformulate the dataset using latent variables\n",
    "Recreate the dataset with the latent variables and obtain the range of such variables to measure our design space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- New database ---\n",
    "# Choose whether to process the old database. It will be saved in ./data/\n",
    "\n",
    "process_database = False\n",
    "if process_database:\n",
    "    def encode_coords(row, model):\n",
    "        \"\"\"\n",
    "        Encodes the coordinates of a row of the dataset. Uses an autoencoder model to encode the coordinates.\n",
    "        Inputs:\n",
    "        - row: A row of the dataset. Contains 30 coordinate points.\n",
    "        - model: An autoencoder model.\n",
    "        Outputs:\n",
    "        - encoded: The encoded coordinates of the row.\n",
    "        \"\"\"\n",
    "        return model.encode(torch.Tensor(row.iloc[:][0:30].values.reshape(-1,30)).to(device)).detach().cpu().numpy()[0] \n",
    "\n",
    "    # --- New dataset ---\n",
    "    airfoil_df_encoded = airfoil_df_norm.copy(deep=True)\n",
    "    # Drop the columns containing original coordinates\n",
    "    airfoil_df_encoded = airfoil_df_encoded[airfoil_df_encoded.columns.drop(list(airfoil_df_encoded.filter(regex='^y')))]\n",
    "\n",
    "    encoded_coords = airfoil_df_norm.apply(encode_coords, axis=1, model=model)\n",
    "    for i in range(1, num_features+1):\n",
    "        airfoil_df_encoded[f\"feat{i}\"] = [j[i-1] for j in encoded_coords]\n",
    "\n",
    "    cols = airfoil_df_encoded.columns.tolist()\n",
    "    cols = cols[-num_features:] + cols[:-num_features]\n",
    "    airfoil_df_encoded = airfoil_df_encoded[cols]\n",
    "    airfoil_df_encoded.to_csv(\"./data/NACA4Digit_Dataset15Point_encoded\"+str(num_features)+\".csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze design space\n",
    "Get bounds for each of the latent variables: minimum and maximum values and width of the variable space ($\\Delta$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input dataset (.csv) name and path\n",
    "data_folder = './data/'\n",
    "dataset_name = 'NACA4Digit_Dataset15Point_encoded'+str(num_features)+'.csv'\n",
    "\n",
    "# Import dataset\n",
    "airfoil_df_encoded = pd.read_csv(data_folder + dataset_name)\n",
    "airfoil_df_encoded = airfoil_df_encoded.drop('Unnamed: 0', axis=1)    # Remove first column, counter\n",
    "# Data scaler fitting\n",
    "scaler_enc = MinMaxScaler()\n",
    "scaler_enc.fit(airfoil_df_encoded)\n",
    "\n",
    "# Assemble a DataFrame with all the minimum and maximum values of each column\n",
    "# For normalization and de-normalization. Gives an idea of the bounds.\n",
    "scaler_enc_bounds = pd.DataFrame(data = np.stack([scaler_enc.feature_names_in_, scaler_enc.data_min_, scaler_enc.data_max_], axis=1), columns=['property', 'min', 'max'])\n",
    "scaler_enc_bounds['delta'] = [(scaler_enc_bounds[scaler_enc_bounds['property']==feat]['max'].values[0]-scaler_enc_bounds[scaler_enc_bounds['property']==feat]['min'].values[0]) for feat in scaler_enc_bounds['property'].values]\n",
    "scaler_enc_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and plot an airfoil\n",
    "Use the decoder part of the AE network to generate and plot an airfoil from its randomly generated latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_random = np.array([np.random.rand()*(scaler_enc_bounds[scaler_enc_bounds['property']==f'feat{i}']['delta'].values[0])+scaler_enc_bounds[scaler_enc_bounds['property']==f'feat{i}']['min'].values[0] for i in range(1, num_features+1)])\n",
    "print(latent_random.reshape(-1,num_features)[0])\n",
    "print(type(latent_random), latent_random.shape)\n",
    "invented = denormalize_data(np.concatenate((model.decode(torch.Tensor(latent_random.reshape(-1,num_features)).to(device)).detach().cpu().numpy()[0], dummy), axis=0),scaler)\n",
    "airfoil_plot(invented, label=\"Invented\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe0a3989b890fde71862de81e2f587f17cb49601957bca599f33cd6f45bc5793"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorchML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
